
@misc{goodfellow_generative_2014,
	title = {Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1406.2661},
	doi = {10.48550/arXiv.1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	number = {{arXiv}:1406.2661},
	publisher = {{arXiv}},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	urldate = {2025-01-09},
	date = {2014-06-10},
	langid = {american},
	eprinttype = {arxiv},
	eprint = {1406.2661 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:I\:\\Zotero\\storage\\5L8QA9KM\\Goodfellow 等 - 2014 - Generative Adversarial Networks.pdf:application/pdf;Snapshot:I\:\\Zotero\\storage\\F7U6GB84\\1406.html:text/html},
}

@misc{radford_unsupervised_2016,
	title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1511.06434},
	doi = {10.48550/arXiv.1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks ({CNNs}) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with {CNNs} has received less attention. In this work we hope to help bridge the gap between the success of {CNNs} for supervised learning and unsupervised learning. We introduce a class of {CNNs} called deep convolutional generative adversarial networks ({DCGANs}), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	number = {{arXiv}:1511.06434},
	publisher = {{arXiv}},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	urldate = {2025-01-09},
	date = {2016-01-07},
	eprinttype = {arxiv},
	eprint = {1511.06434 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:I\:\\Zotero\\storage\\AG24B8ET\\Radford 等 - 2016 - Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.pdf:application/pdf;Snapshot:I\:\\Zotero\\storage\\JFC9TASV\\1511.html:text/html},
}

@inproceedings{pan_drag_2023,
	title = {Drag Your {GAN}: Interactive Point-based Manipulation on the Generative Image Manifold},
	url = {http://arxiv.org/abs/2305.10973},
	doi = {10.1145/3588432.3591500},
	shorttitle = {Drag Your {GAN}},
	abstract = {Synthesizing visual content that meets users' needs often requires flexible and precise controllability of the pose, shape, expression, and layout of the generated objects. Existing approaches gain controllability of generative adversarial networks ({GANs}) via manually annotated training data or a prior 3D model, which often lack flexibility, precision, and generality. In this work, we study a powerful yet much less explored way of controlling {GANs}, that is, to "drag" any points of the image to precisely reach target points in a user-interactive manner, as shown in Fig.1. To achieve this, we propose {DragGAN}, which consists of two main components: 1) a feature-based motion supervision that drives the handle point to move towards the target position, and 2) a new point tracking approach that leverages the discriminative generator features to keep localizing the position of the handle points. Through {DragGAN}, anyone can deform an image with precise control over where pixels go, thus manipulating the pose, shape, expression, and layout of diverse categories such as animals, cars, humans, landscapes, etc. As these manipulations are performed on the learned generative image manifold of a {GAN}, they tend to produce realistic outputs even for challenging scenarios such as hallucinating occluded content and deforming shapes that consistently follow the object's rigidity. Both qualitative and quantitative comparisons demonstrate the advantage of {DragGAN} over prior approaches in the tasks of image manipulation and point tracking. We also showcase the manipulation of real images through {GAN} inversion.},
	pages = {1--11},
	booktitle = {Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Proceedings},
	author = {Pan, Xingang and Tewari, Ayush and Leimkühler, Thomas and Liu, Lingjie and Meka, Abhimitra and Theobalt, Christian},
	urldate = {2025-01-09},
	date = {2023-07-23},
	eprinttype = {arxiv},
	eprint = {2305.10973 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	file = {Preprint PDF:I\:\\Zotero\\storage\\D659MC3T\\Pan 等 - 2023 - Drag Your GAN Interactive Point-based Manipulation on the Generative Image Manifold.pdf:application/pdf;Snapshot:I\:\\Zotero\\storage\\HY3CFSMI\\2305.html:text/html},
}

@misc{ho_denoising_2020,
	title = {Denoising Diffusion Probabilistic Models},
	url = {http://arxiv.org/abs/2006.11239},
	doi = {10.48550/arXiv.2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional {CIFAR}10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art {FID} score of 3.17. On 256x256 {LSUN}, we obtain sample quality similar to {ProgressiveGAN}. Our implementation is available at https://github.com/hojonathanho/diffusion},
	number = {{arXiv}:2006.11239},
	publisher = {{arXiv}},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	urldate = {2025-01-09},
	date = {2020-12-16},
	eprinttype = {arxiv},
	eprint = {2006.11239 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:I\:\\Zotero\\storage\\QJ6TWWG8\\Ho 等 - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf;Snapshot:I\:\\Zotero\\storage\\DWQGD7YM\\2006.html:text/html},
}

@misc{chen_overview_2024,
	title = {An Overview of Diffusion Models: Applications, Guided Generation, Statistical Rates and Optimization},
	url = {http://arxiv.org/abs/2404.07771},
	doi = {10.48550/arXiv.2404.07771},
	shorttitle = {An Overview of Diffusion Models},
	abstract = {Diffusion models, a powerful and universal generative {AI} technology, have achieved tremendous success in computer vision, audio, reinforcement learning, and computational biology. In these applications, diffusion models provide flexible high-dimensional data modeling, and act as a sampler for generating new samples under active guidance towards task-desired properties. Despite the significant empirical success, theory of diffusion models is very limited, potentially slowing down principled methodological innovations for further harnessing and improving diffusion models. In this paper, we review emerging applications of diffusion models, understanding their sample generation under various controls. Next, we overview the existing theories of diffusion models, covering their statistical properties and sampling capabilities. We adopt a progressive routine, beginning with unconditional diffusion models and connecting to conditional counterparts. Further, we review a new avenue in high-dimensional structured optimization through conditional diffusion models, where searching for solutions is reformulated as a conditional sampling problem and solved by diffusion models. Lastly, we discuss future directions about diffusion models. The purpose of this paper is to provide a well-rounded theoretical exposure for stimulating forward-looking theories and methods of diffusion models.},
	number = {{arXiv}:2404.07771},
	publisher = {{arXiv}},
	author = {Chen, Minshuo and Mei, Song and Fan, Jianqing and Wang, Mengdi},
	urldate = {2025-01-09},
	date = {2024-04-11},
	eprinttype = {arxiv},
	eprint = {2404.07771 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Statistics Theory, Statistics - Statistics Theory},
	file = {Preprint PDF:I\:\\Zotero\\storage\\3YC7GDM3\\Chen 等 - 2024 - An Overview of Diffusion Models Applications, Guided Generation, Statistical Rates and Optimization.pdf:application/pdf;Snapshot:I\:\\Zotero\\storage\\NNF4YNY8\\2404.html:text/html},
}

@misc{tian_visual_2024,
	title = {Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction},
	url = {http://arxiv.org/abs/2404.02905},
	doi = {10.48550/arXiv.2404.02905},
	shorttitle = {Visual Autoregressive Modeling},
	abstract = {We present Visual {AutoRegressive} modeling ({VAR}), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine "next-scale prediction" or "next-resolution prediction", diverging from the standard raster-scan "next-token prediction". This simple, intuitive methodology allows autoregressive ({AR}) transformers to learn visual distributions fast and generalize well: {VAR}, for the first time, makes {GPT}-like {AR} models surpass diffusion transformers in image generation. On {ImageNet} 256x256 benchmark, {VAR} significantly improve {AR} baseline by improving Frechet inception distance ({FID}) from 18.65 to 1.73, inception score ({IS}) from 80.4 to 350.2, with around 20x faster inference speed. It is also empirically verified that {VAR} outperforms the Diffusion Transformer ({DiT}) in multiple dimensions including image quality, inference speed, data efficiency, and scalability. Scaling up {VAR} models exhibits clear power-law scaling laws similar to those observed in {LLMs}, with linear correlation coefficients near -0.998 as solid evidence. {VAR} further showcases zero-shot generalization ability in downstream tasks including image in-painting, out-painting, and editing. These results suggest {VAR} has initially emulated the two important properties of {LLMs}: Scaling Laws and zero-shot task generalization. We have released all models and codes to promote the exploration of {AR}/{VAR} models for visual generation and unified learning.},
	number = {{arXiv}:2404.02905},
	publisher = {{arXiv}},
	author = {Tian, Keyu and Jiang, Yi and Yuan, Zehuan and Peng, Bingyue and Wang, Liwei},
	urldate = {2025-01-09},
	date = {2024-06-10},
	eprinttype = {arxiv},
	eprint = {2404.02905 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:I\:\\Zotero\\storage\\H78YFITN\\Tian 等 - 2024 - Visual Autoregressive Modeling Scalable Image Generation via Next-Scale Prediction.pdf:application/pdf;Snapshot:I\:\\Zotero\\storage\\EZXHNPYB\\2404.html:text/html},
}
